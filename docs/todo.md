- [ ] RAG的多实体问题如何解决：爱因斯坦和牛顿在物理学上的贡献有何不同？
- [ ] 多任务学习中互斥任务影响削减：1) model merging; 2) 基于prompt训练
- [ ] 从零构建DS：https://avoid.overfit.cn/post/ac6d4be0a234412ea00032737365638c#
- [ ] MCP：model context protocol
- [ ] function call
- [x] triton框架
- [x] ollama, tensorRT
- [x] langchain (Language Chain)
- [ ] dify(do it for you)
- [x] ragflow
- [ ] haystack
- [ ] 阿里百炼
- [x] docker
- [x] kubernetes (k8s)
- [ ] 联邦学习
- [ ] Fisher information matrix
- [ ] Hessian matrix
- [ ] QK-Norm, 在W_Q/K/V输入前进行LN操作
    - Scaling vision transformers to 22 billion parameters
    - Small-scale proxies for large-scale transformer training instabilities
- [ ] bits-per-byte, BPB, The Pile: An 800GB Dataset of Diverse Text for Language Modeling
- [x] scaling laws
    - Deep learning scaling is predictable, empirically
    - Scaling laws for autoregressive generative modeling
    - Training compute-optimal large language models, IsoFLOP固定算力
    - Scaling laws for neural language models
- [x] DARE: Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch
- [x] linear attention
    - Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention
    - Linformer: Self-Attention with Linear Complexity
- [x] MRL paper: Matryoshka Representation Learning, 一般mrl → norm
- [x] Slimmable Neural Networks
- [ ] Reducing activation recomputation in large transformer models.
- [ ] 3D parallel
- [x] pipedream, pipedream-2bw
- [x] Zero bubble pipeline parallelism
- [ ] Device Capacity Factor
- [ ] ROI Pooling, Region of Interest: 1) region proposal; 2) pooling sections; 3) max_pooling of sections
    - 将feature map划分为 H*W 个区域
    - 通过max_pooling得到 H*W 大小的特征图
- [ ] Persona Hub：Scaling Synthetic Data Creation with 1,000,000,000 Personas
- [ ] dashscope for alibaba; openai for openai; requests for glm4
- [ ] yolo家族， SSD（Single Shot MultiBox Detector）多尺度特征图预测，RetinaNet
- [ ] Faster R-CNN
- [x] GoogleNet=Inception v1，Inception家族，Inception-ResNet
- [ ] AlexNet: 引入dropout和ReLU激活函数
- [ ] VGGNet：全部使用3*3的kernel，减少参数，增加非线性
- [ ] ResNet-50：提出残差连接
- [ ] FCN（Fully Convolutional Network）
- [ ] ConvNeXt，MobileNet，ShuffleNet, EfficientNet，3D CNN，PointNet 
- [ ] DETR
- [ ] openCV
- [ ] cv中常见的数据增强方案：crop 对单张输入图像裁剪
- [ ] xfyun for xunfei
- [ ] Universal transformers. 类似 RNN 的权重共享，所有时间步共享同一组 Self-Attention 和 FFN 参数
- [x] mGTE
- [x] bge, bge-m3
- [x] jina embedding，jina ai推出
- [x] e5 embedding，intfloat团队研发
- [x] xlm-roberta，fair研发
- [ ] https://github.com/castorini/pyserini
- [x] cot, cot-sc, tot, got
- [x] transformers.optmization, transformers.Trainer
- [ ] q-learning DQN, DDPG不用重要性采样
- [x] DPO, PPO, GRPO, DAPO
- [ ] GLM-4.5, Kimi-K2, Qwen
- [ ] tf.metrics, torchmetrics
- [ ] 文档chunk方案
        - Fixed-size window Chunking + overlapping, charatertextsplitter
        - 结构化分块：markdown、html、pdf等结构化文档
- [x] 对比学习负样本选择方案, improved infonce loss
- [x] 文档检索top-k后还要进行重排reranking，可在结合搜索推荐中的重排技术
- [ ] sft roberta with multiple sequence concurrent with customized attention mask
- [x] DeepSeek-2
- [x] DeepSeek-3 https://zhuanlan.zhihu.com/p/16323685381
- [ ] 免费 gpt-4: https://gpt.xfai.online/list/#/home
- [x] instruct gpt: Training language models to follow instructions with human feedback
- [x] GPTQ
- [x] Atom: Low-bit quantization for efficient and accurate LLM serving
- [ ] MoE: https://zhuanlan.zhihu.com/p/669312652
- [ ] [huggingface leaderboard](https://huggingface.co/spaces?q=leaderboard)
- [ ] [code with paper leaderboard](https://paperswithcode.com/sota)
- [ ] [大模型面试](https://zhuanlan.zhihu.com/p/691588703)
- [x] NSA: natively trainable sparse Attention, hierarchical attention
- [ ] Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct
- [ ] Math-shepherd: Verify and reinforce llms step-by-step without human annotations
- [x] vllm: Efficient Memory Management for Large Language Model Serving with PagedAttention
- [x] HAI-LLM framework（higher flyer）
- [ ] Colossal-AI:  A Unified Deep Learning System For Large-Scale Parallel Training
- [ ] accelerate config
- [ ] 模型幻觉hallucination
- [ ] 持续学习，避免灾难性遗忘
    - 正则化、数据放回、增量学习、adapter网络，使用pre model软标签数据参与训练等
- [x] [gradient checkpointing, activation checkpointing](https://www.bilibili.com/video/BV1nJ4m1M7Qw/?spm_id_from=333.1387.search.video_card.click&vd_source=782e4c31fc5e63b7cb705fa371eeeb78): Training Deep Nets with Sublinear Memory Cost
- [x] Gradient Checkpointing，[gif](https://pic3.zhimg.com/v2-1679b74a85687cdb250e532931bb266a_b.webp), reduce the activation memory by approximately the square root of the total activations at the expense of 33% re-computation overhead
- [ ] length normalization
- [ ] lora with diffusion model 
- [x] LDA潜在迪利克雷分布，b站视频 LDA主题模型
- [ ] LSA/PLSA
- [ ] Cholesky分解
- [ ] [odds，logit，ligitis](https://zhuanlan.zhihu.com/p/435912211)
- [ ] [GBDT + LR](https://www.cnblogs.com/wkang/p/9657032.html)
- [ ] Restricted Boltzmann Machines (RBM)
- [ ] A/B test
- [ ] TF-IDF_j, MI_{a, b, c, d}
- tensorflow 1.x中在梯度下降时如何设置L1，L2正则化约束
    ```python
    with tf.variable_scope("layer1", regularizer=l2_regularizer):
        xxx
    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)
    total_loss = cross_entropy_loss + sum(reg_losses)
    ```
- 推荐系统攻击
- [推荐系统论文笔记](https://github.com/Doragd/Algorithm-Practice-in-Industry/blob/main/%E6%90%9C%E5%B9%BF%E6%8E%A8%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%E4%B8%B2%E8%AE%B2.md#%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0)
    - [x] 推荐系统论文精读
    - [x] 经典推荐算法学习
    - [x] 推荐系统与深度学习论文笔记
- https://km.netease.com/v4/detail/blog/223053  
- https://readpaper.feishu.cn/docx/CrMGdSVPKow5d1x1XQMcJioRnQe
