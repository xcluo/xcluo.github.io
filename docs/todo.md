- [ ] RAG的多实体问题如何解决：爱因斯坦和牛顿在物理学上的贡献有何不同？
- [x] triton框架
- [x] ollama, tensorRT
- [x] langchain (Language Chain)
- [ ] dify(do it for you)
- [x] ragflow
- [ ] haystack
- [ ] 阿里百炼
- [x] docker
- [ ] kubernetes (k8s)
- [ ] 联邦学习
- [ ] dashscope for alibaba; openai for openai; requests for glm4
- [ ] xfyun for xunfei
- [x] gte (General Text Embeddings)，阿里巴巴达摩院推出
- [x] jina embedding，jina ai推出
- [x] e5 embedding，intfloat团队研发
- [x] xlm-roberta，fair研发
- [ ] https://github.com/castorini/pyserini
- [x] transformers.optmization, transformers.Trainer
- [ ] tf.metrics, torchmetrics
- [ ] 文档chunk方案
        - Fixed-size window Chunking + overlapping, charatertextsplitter
        - 结构化分块：markdown、html、pdf等结构化文档
- [ ] 对比学习负样本选择方案
- [x] 文档检索top-k后还要进行重排reranking，可在结合搜索推荐中的重排技术
- [ ] 量化float2float，float2int
- Frobenius范数，次可加性$\Vert A+B \Vert_{F}\le \Vert A \Vert_F + \Vert B \Vert_F$，空间向量相加，两边之和大于第三边
- [ ] sft roberta with multiple sequence concurrent with customized attention mask
- [x] DeepSeek v2
- [ ] DeepSeek v3 https://zhuanlan.zhihu.com/p/16323685381
- [ ] 免费 gpt-4: https://gpt.xfai.online/list/#/home
- [ ] instruct gpt: Training language models to follow instructions with human feedback
- [x] Kvquant: Towards 10 million context length LLM inference with KV cache quantization
- [x] zeroquant, 
- [x] smooth quant
- [ ] Atom: Low-bit quantization for efficient and accurate LLM serving
- [ ] MoE: https://zhuanlan.zhihu.com/p/669312652
- [ ] q-learning DQN, DDPG不用重要性采样
- [x] DPO, PPO, GRPO, DAPO
- [ ] [huggingface leaderboard](https://huggingface.co/spaces?q=leaderboard)
- [ ] [code with paper leaderboard](https://paperswithcode.com/sota)
- [ ] [大模型面试](https://zhuanlan.zhihu.com/p/691588703)
- [x] NSA: natively trainable sparse Attention
- [ ] Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct
- [ ] Math-shepherd: Verify and reinforce llms step-by-step without human annotations
- [x] vllm: Efficient Memory Management for Large Language Model Serving with PagedAttention
- [x] DeepSpeed（Zero Redundancy Optimizer）、Megatron-LM、HAI-LLM framework（higher flyer）
- [x] zero-dp：zero 1,2,3 用通信换内存，zero-3引入了额外的通信开销用于forward和backward（用了就扔）
- [x] zero-r, zero-offload, zero-infinity
- [ ] ring all-reduce：梯度同步；ring all-gather：参数更新和加载
- [ ] 模型DP时，多个数据loss结果会进行交互all reduce
- [ ] accelerate config
- [ ] 模型幻觉hallucination
- [ ] 持续学习，避免灾难性遗忘
    - 正则化、数据放回、增量学习、adapter网络，使用pre model软标签数据参与训练等
- [ ] MCP：model context protocol
- [x] [gradient checkpointing](https://www.bilibili.com/video/BV1nJ4m1M7Qw/?spm_id_from=333.1387.search.video_card.click&vd_source=782e4c31fc5e63b7cb705fa371eeeb78): Training Deep Nets with Sublinear Memory Cost
- [x] Gradient Checkpointing，[gif](https://pic3.zhimg.com/v2-1679b74a85687cdb250e532931bb266a_b.webp)
- [ ] label smoothing
- [ ] 在embedding 层后添加layer normalization，有利于提升训练稳定性:但可能会带来很大的性能损失.
- [ ] Prefix Tuning: 基于prefix内容进行回答
- [ ] Prompt Tuning: 自动化提示工程，改动prompt完成问题的回答
- [ ] p-tuning v1/v2
- [ ] prompt engineering
- [ ] lora with diffusion model 
- [x] KV cache：将L层K与V进行缓存以执行Attention，各层矩阵为 `k.shape = (bs, n_heads, seq_len, head_dim)`
- [x] LDA潜在迪利克雷分布，b站视频 LDA主题模型
- [ ] LSA/PLSA
- [ ] Cholesky分解
- [ ] [odds，logit，ligitis](https://zhuanlan.zhihu.com/p/435912211)
- [ ] [GBDT + LR](https://www.cnblogs.com/wkang/p/9657032.html)
- [ ] Restricted Boltzmann Machines (RBM)
- [ ] A/B test
- [ ] TF-IDF_j, MI_{a, b, c, d}
- [x] Attention softmax后除以$\sqrt{d_k}$是因为权重矩阵中每个元素都是通过两个(1， d_k)方差为1的向量相乘得到的，基于正态分布累加后的标准差公式可知该值方差变为$\sqrt{d_k}$，因此执行该操作，不除以$\sqrt{d_k}$，根据softmax函数曲线，softmax结果表现更倾向于one-hot分布，[会带来梯度消失问题](https://spaces.ac.cn/archives/8620/comment-page-4#comment-24076)
- tensorflow 1.x中在梯度下降时如何设置L1，L2正则化约束
    ```python
    with tf.variable_scope("layer1", regularizer=l2_regularizer):
        xxx
    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)
    total_loss = cross_entropy_loss + sum(reg_losses)
    ```
- 推荐系统攻击
- [推荐系统论文笔记](https://github.com/Doragd/Algorithm-Practice-in-Industry/blob/main/%E6%90%9C%E5%B9%BF%E6%8E%A8%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%E4%B8%B2%E8%AE%B2.md#%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0)
    - [x] 推荐系统论文精读
    - [x] 经典推荐算法学习
    - [x] 推荐系统与深度学习论文笔记
- https://km.netease.com/v4/detail/blog/223053  
- https://readpaper.feishu.cn/docx/CrMGdSVPKow5d1x1XQMcJioRnQe
