### BART
> 论文：Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension  
> BART: **B**idirectional and **A**uto-**R**egressive
**T**ransformers  
> Facebook AI, ACL 2020

#### 工作要点
- corrupt text with an arbitrary noising function;
- seq-to-seq model to reconstruct the original text