### FGSM
> 论文：Explaining and Harnessing Adversarial Examples  
> FGSM：**F**ast **G**radient **S**ign **M**ethod  
> Google, ICLA 2015

#### 工作要点
- fgsm attack，在梯度方向些微扰动即可轻易使模型误判

### FGM
> 论文：Adversarial Training Methods for Semi-supervised Text Classification  
> FGM：**F**ast **G**radient **M**ethod  
> Github：[adversarial_text](https://github.com/tensorflow/models/tree/master/research/adversarial_text)  
> Preferred Networks & Google & OpenAI, ICLA 2017

#### 工作要点
- 第一个将VAT应用至文本领域
- dropout followed by FGM performs the best

### PGD
> 论文：Towards Deep Learning Models Resistant to Adversarial Attacks  
> PGD：**P**rojected **G**radient **D**escent  
> MIT, ICLA 2018



### SMART
> 论文：SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization  
> SMART：**SM**oothness-inducing **A**dversarial **R**egularization  
> Microsoft Dynamics 365 AI, ACL 2020