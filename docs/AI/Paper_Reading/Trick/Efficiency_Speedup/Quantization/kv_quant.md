## KVQuant
> 论文：KVQuant: Towards 10 million context length LLM inference with KV cache quantization  
> UC Berkeley & Stanford University & Independent Researcher & UC San Diego 2023 Sep, SIGOPS 2023


### 主要内容