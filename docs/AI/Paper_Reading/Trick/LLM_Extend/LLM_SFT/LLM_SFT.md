由于模型规模过大，常规情况下的算力无法支持全量微调，因此需要其它部分微调方法（即高效参数微调法PEFT，Parameter-Effecient Fine-Tuning）实现模型的transfer learning。

### PEFT

#### [LoRA](lora.md)
- [Mixture of LoRA Experts (MoLE)](https://openreview.net/pdf?id=uWvKBCYh4S)
- [Higher Layers Need More LoRA Experts](https://arxiv.org/pdf/2402.08562v1)
- [LoRA insight experiments](https://lightning.ai/pages/community/lora-insights/#toc12)
- [Practical Tips when using LoRA](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms)
- [LoRA servey](https://towardsdatascience.com/an-overview-of-the-lora-family-515d81134725)

#### [AdaLoRA](adalora.md)

#### [PiSSA](pissa.md)


#### [DoRA](dora.md)

#### [LoRA-GA](lora-ga.md)

#### [QLoRA](qlora.md)

### Instrument Finetune

### Prompt Finetune