

### tokenization
[`FullTokenizer`](https://github.com/google-research/bert/blob/master/tokenization.py#L161C11-L161C11)æ’å…¥`special_token`

1. variety_span
2. ç‰¹æ®Šå­—ç¬¦åˆ‡æ¢ä¸ºå·²ç»æ›¿æ¢çš„æŸä¸ªtokenä½œä¸º`relay_token`ï¼ˆ`relay_token`ä¸ºå•å­—ç¬¦unicodeä¸”å·¦å³å„å¢åŠ ä¸€ä¸ªç©ºæ ¼ä»¥ç¡®ä¿æ•´ä½“åˆ†è¯ä¸º`relay_token`ï¼Œè€Œä¸æ˜¯`##relay_token`ï¼‰
3. tokenize
4. tokenized tokensè¿˜åŸï¼Œå³`relay_token` â†’ map â†’ `target_token`
  > å¯ä¸recoverï¼Œç›´æ¥ç”¨è¯¥tokenè¡¨ç¤ºç‰¹æ®Šè¯­ä¹‰
!!! info 
    - æ¯ç§åŠŸèƒ½çš„`special_token`å°½å¯èƒ½ç‹¬ç«‹ï¼Œæ¯”å¦‚`[SPACE]` ä¸å’Œ `[PAD]`ã€`[SEP]`å…±ç”¨


### FullTokenizerä¼˜åŒ–

#### å»é™¤å£°è°ƒä¸Šä¸‹æ ‡
```python title="BasicTokenizer._run_strip_accents"
# å»é™¤å­—ç¬¦å£°è°ƒ "Ä Ã¡ Ç Ã "   -> "a a a a"
def _run_strip_accents(text):
    """Strips accents from a piece of text."""
    text = unicodedata.normalize("NFD", text)
    output = []
    for char in text:
      cat = unicodedata.category(char)
      if cat == "Mn":
        continue
      output.append(char)
    return "".join(output)
```

#### é¿å…oovæ¯’æ€§æ‰©æ•£
```python title="WordpieceTokenizer.tokenize"
# ä¼˜åŒ–ï¼šé¿å…BPEåˆ†è¯oovæ¯’æ€§æ‰©æ•£ç°è±¡ï¼Œe.g., "ç«ç‘°èŠ±ğ–Ÿlá´¤æœµå‘æ—¥è‘µ3â°‹7æœµ"
## [ç«ï¼Œç‘°ï¼ŒèŠ±ï¼Œunkï¼Œæœµï¼Œå‘ï¼Œæ—¥ï¼Œè‘µï¼Œunkï¼Œæœµ] -> 
## [ç«ï¼Œç‘°ï¼ŒèŠ±ï¼Œunk, 1, unkï¼Œæœµï¼Œå‘ï¼Œæ—¥ï¼Œè‘µï¼Œ3, unk, ##7ï¼Œæœµ]
def tokenize(self, text):
    text = convert_to_unicode(text)

    output_tokens = []
    for token in whitespace_tokenize(text):
      chars = list(token)
      if len(chars) > self.max_input_chars_per_word:
        output_tokens.append(self.unk_token)
        continue

      # is_bad = False
      start = 0
      sub_tokens = []
      while start < len(chars):
        end = len(chars)
        cur_substr = None
        while start < end:
          substr = "".join(chars[start:end])
          if start > 0:
            substr = "##" + substr
          if substr in self.vocab:
            cur_substr = substr
            break
          end -= 1
        # if cur_substr is None:
        #   is_bad = True
        #   break
        # sub_tokens.append(cur_substr)
        # start = end

        if cur_substr is None:
            if len(sub_tokens) == 0 \
              or sub_tokens[-1] != self.unk_token:  # unify multiple-unk_token or one_unk-to-one_token
                sub_tokens.append(self.unk_token)
            start += 1
        else:
            sub_tokens.append(cur_substr)
            start = end

      # if is_bad:
      #   output_tokens.append(self.unk_token)
      # else:
      #   output_tokens.extend(sub_tokens)

      output_tokens.extend(sub_tokens)
    return output_tokens
```