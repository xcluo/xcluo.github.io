

### tokenization
[`FullTokenizer`](https://github.com/google-research/bert/blob/master/tokenization.py#L161C11-L161C11)æ’å…¥`special_token`

1. variety_span_replace
2. ç‰¹æ®Šå­—ç¬¦åˆ‡æ¢ä¸ºå·²ç»æ›¿æ¢çš„æŸä¸ªtokenä½œä¸º`relay_token`ï¼ˆ`relay_token`ä¸ºå•å­—ç¬¦unicodeä¸”å·¦å³å„å¢åŠ ä¸€ä¸ªç©ºæ ¼ä»¥ç¡®ä¿æ•´ä½“åˆ†è¯ä¸º`relay_token`ï¼Œè€Œä¸æ˜¯`##relay_token`ï¼‰
3. tokenize
4. tokenized tokensè¿˜åŸï¼Œå³`relay_token` â†’ map â†’ `target_token`
  > recoverç¯èŠ‚ä¸æ˜¯å¿…éœ€çš„ï¼Œå¯ç›´æ¥ç”¨è¯¥tokenè¡¨ç¤ºç‰¹æ®Šè¯­ä¹‰
!!! info 
    - æ¯ç§åŠŸèƒ½çš„`special_token`å°½å¯èƒ½ç‹¬ç«‹ï¼Œæ¯”å¦‚`[SPACE]` ä¸å’Œ `[PAD]`ã€`[SEP]`å…±ç”¨
    - åºåˆ—å„éƒ¨åˆ†åˆ’åˆ†token + é€çº§æ‹†åˆ†tokenï¼Œ"å…­æœˆå››ri" -> `[å…­, æœˆ, å››, ri]`è€Œä¸æ˜¯`[å…­, æœˆ, å››, r, i]`
    ```python                 
      # 1. _tokenize_chinese_chars
      # 2. whitespace_tokenize
      # 3. _run_split_on_punc
      # 4. å…ˆtoken-levelï¼Œå†char-level
      for token in above_token_seq:
        if token in vocab:
          output_tokens.append(token)
        else:
          for c in vocab:
            output_tokens.append(c if c in vocab else unk_token)
    ```


### FullTokenizerä¼˜åŒ–

#### è°¨æ…ä½¿ç”¨å£°è°ƒä¸Šä¸‹æ ‡å»é™¤å‡½æ•°
å°†æ–‡æœ¬ç»è¿‡NFDåˆ†è§£ä¸º`åŸºæœ¬å­—ç¬¦ + é‡éŸ³ç¬¦å·`ååªä¿ç•™åŸºç¡€å­—ç¬¦ï¼Œå¦‚`"Ä Ã¡ Ç Ã "   -> "a a a a"`  

  - [ ] å¯èƒ½ç›´æ¥ignoreé‡éŸ³ç¬¦å·ä¼šé€ æˆä¿¡æ¯è¯­ç´ ç¼ºå¤±ï¼ˆå¦‚å½¢ä¼¼çš„æ•°å­—ã€å­—æ¯ç­‰å˜ç§å­—ç¬¦ï¼‰
```python title="BasicTokenizer._run_strip_accents"
# å»é™¤å­—ç¬¦å£°è°ƒ "Ä Ã¡ Ç Ã "   -> "a a a a"
def _run_strip_accents(text):
    """Strips accents from a piece of text."""
    text = unicodedata.normalize("NFD", text)
    output = []
    for char in text:
      cat = unicodedata.category(char)
      if cat == "Mn":
        continue
      output.append(char)
    return "".join(output)
```

#### é¿å…oovæ¯’æ€§æ‰©æ•£
```python title="WordpieceTokenizer.tokenize"
# ä¼˜åŒ–ï¼šé¿å…BPEåˆ†è¯oovæ¯’æ€§æ‰©æ•£ç°è±¡ï¼Œe.g., "ç«ç‘°èŠ±ğ–Ÿlá´¤æœµå‘æ—¥è‘µ3â°‹7æœµ"
## [ç«ï¼Œç‘°ï¼ŒèŠ±ï¼Œunkï¼Œæœµï¼Œå‘ï¼Œæ—¥ï¼Œè‘µï¼Œunkï¼Œæœµ] -> 
## [ç«ï¼Œç‘°ï¼ŒèŠ±ï¼Œunk, 1, unkï¼Œæœµï¼Œå‘ï¼Œæ—¥ï¼Œè‘µï¼Œ3, unk, ##7ï¼Œæœµ]
def tokenize(self, text):
    text = convert_to_unicode(text)

    output_tokens = []
    for token in whitespace_tokenize(text):
      chars = list(token)
      if len(chars) > self.max_input_chars_per_word:
        output_tokens.append(self.unk_token)
        continue

      # is_bad = False
      start = 0
      sub_tokens = []
      while start < len(chars):
        end = len(chars)
        cur_substr = None
        while start < end:
          substr = "".join(chars[start:end])
          if start > 0:
            substr = "##" + substr
          if substr in self.vocab:
            cur_substr = substr
            break
          end -= 1
        # if cur_substr is None:
        #   is_bad = True
        #   break
        # sub_tokens.append(cur_substr)
        # start = end

        if cur_substr is None:
            if len(sub_tokens) == 0 \
              or sub_tokens[-1] != self.unk_token:  # unify multiple-unk_token or one_unk-to-one_token
                sub_tokens.append(self.unk_token)
            start += 1
        else:
            sub_tokens.append(cur_substr)
            start = end

      # if is_bad:
      #   output_tokens.append(self.unk_token)
      # else:
      #   output_tokens.extend(sub_tokens)

      output_tokens.extend(sub_tokens)
    return output_tokens
```